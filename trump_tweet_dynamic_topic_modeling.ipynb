{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a1018-23e6-4e73-9a7d-82a93b6b3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob\n",
    "from bertopic import BERTopic\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download necessary NLTK data (if not already done)\n",
    "'''\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b81726-aafd-47d4-9984-db135a98f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump_df = pd.read_csv(\"../Data/Tweat_data/Trump_election_market_time.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43ca25-f29d-40f4-b06f-63d88fb793c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "print(Trump_df.shape)\n",
    "Trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa4eb2-e575-4217-a36e-f9486ac87719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove RT (retweet) - \"RT\"가 포함된 모든 단어 제거\n",
    "    text = re.sub(r'\\b\\w*RT\\w*\\b', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuations, symbols, special characters, and numbers except space\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['great', 'realdonaldtrump'])\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "Trump_df[\"cleaned_text\"] = Trump_df[\"text\"].apply(clean_text)\n",
    "print(Trump_df.shape)\n",
    "Trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0db71-4f13-4f9f-92f7-759901dd894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\")\n",
    "docs = Trump_df[\"cleaned_text\"].to_list()\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "Trump_df['topic'] = topics\n",
    "Trump_df['topic'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477ec5a-56c4-4050-a68c-c982c0ad81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e063902-c66f-4ff0-83db-80c195ea7ad6",
   "metadata": {},
   "source": [
    "### CASE1: reduce topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456822d-acd8-4067-aed5-5d1cdcd2a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CASE1: reduce topics\n",
    "reduced_new_topics = topic_model.reduce_topics(docs)\n",
    "print(len(reduced_new_topics.topics_))\n",
    "pd.DataFrame(reduced_new_topics.topics_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531c317-d03c-40ac-ba55-88f2a68fa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_new_topics.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21d2d9-4292-45e6-875d-118f5f947d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default='iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de8d78-41af-42a2-8c43-d77b2b2e0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = reduced_new_topics.visualize_topics()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c8421-97de-4010-bea4-283d4a431769",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_new_topics.visualize_barchart(range(0, 10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0a978-61dc-4ff3-9815-d84c472d6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs=docs,\n",
    "                                                timestamps=df['date'].to_list(),\n",
    "                                                global_tuning=True,\n",
    "                                                evolution_tuning=True,\n",
    "                                                nr_bins=20)\n",
    "\n",
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850b14b-2fdd-4233-8bdd-41a80989d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = topic_model.get_document_info(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fe651-d253-44a8-990d-6b883f18978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(1, 15 + 1):\n",
    "    topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "    frequencies = topic_data['Frequency']\n",
    "    timestamps = topic_data['Timestamp']\n",
    "    #total_frequencies += frequencies\n",
    "\n",
    "    gradient = np.gradient(frequencies)  # 빈도의 gradient 계산\n",
    "    plt.plot(timestamps, gradient, label=f'Topic {topic_id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73256a-973c-43a5-8f2e-a2f25c4d181d",
   "metadata": {},
   "source": [
    "### Frequency and Gradient of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469eff7c-954a-475a-9a7b-ddb8400d3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_total_frequency_with_gradient(topics_over_time, top_n_topics=20):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 토픽별 빈도를 저장할 딕셔너리 생성\n",
    "    topic_frequencies = {}\n",
    "\n",
    "    # 상위 n개의 토픽만을 사용하여 빈도를 더합니다.\n",
    "    for topic_id in range(1, top_n_topics + 1):\n",
    "        topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "        timestamps = topic_data['Timestamp']\n",
    "        frequencies = topic_data['Frequency']\n",
    "        \n",
    "        # 각 타임스탬프에서의 빈도를 더합니다.\n",
    "        for timestamp, frequency in zip(timestamps, frequencies):\n",
    "            if timestamp in topic_frequencies:\n",
    "                topic_frequencies[timestamp] += frequency\n",
    "            else:\n",
    "                topic_frequencies[timestamp] = frequency\n",
    "\n",
    "    # 딕셔너리를 데이터프레임으로 변환하여 그래프를 그립니다.\n",
    "    df = pd.DataFrame(list(topic_frequencies.items()), columns=['Timestamp', 'Total Frequency'])\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    plt.plot(df['Timestamp'], df['Total Frequency'])\n",
    "\n",
    "    # 빈도의 gradient를 계산하여 그래프로 표시합니다.\n",
    "    gradient = np.gradient(df['Total Frequency'])\n",
    "    plt.plot(df['Timestamp'], gradient, label='Gradient', linestyle='--')\n",
    "\n",
    "    plt.title('Total Frequency and Gradient of Topics over Time')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Total Frequency / Gradient')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 위 함수를 호출하여 각 토픽에 대한 Frequency를 Timestamp마다 총합하고, gradient를 계산하여 그래프로 그립니다.\n",
    "plot_total_frequency_with_gradient(topics_over_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081c664-395a-4f72-b61c-7e66033bd441",
   "metadata": {},
   "source": [
    "### Tweets + SPY open value & volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c269ddc-0f75-4203-8de8-bd8bbb80528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토픽 빈도 데이터와 날짜별 open_avg 및 volume 데이터 병합 및 플롯\n",
    "def plot_total_frequency_with_spy(topics_over_time, daily_data, top_n_topics=20):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 토픽별 빈도를 저장할 딕셔너리 생성\n",
    "    topic_frequencies = {}\n",
    "\n",
    "    # 상위 n개의 토픽만을 사용하여 빈도를 더합니다.\n",
    "    for topic_id in range(1, top_n_topics + 1):\n",
    "        topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "        timestamps = topic_data['Timestamp']\n",
    "        frequencies = topic_data['Frequency']\n",
    "        \n",
    "        # 각 타임스탬프에서의 빈도를 더합니다.\n",
    "        for timestamp, frequency in zip(timestamps, frequencies):\n",
    "            if timestamp in topic_frequencies:\n",
    "                topic_frequencies[timestamp] += frequency\n",
    "            else:\n",
    "                topic_frequencies[timestamp] = frequency\n",
    "\n",
    "    # 딕셔너리를 데이터프레임으로 변환하여 그래프를 그립니다.\n",
    "    df = pd.DataFrame(list(topic_frequencies.items()), columns=['Timestamp', 'Total Frequency'])\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    \n",
    "    # 날짜별로 정리된 SPY 데이터와 병합\n",
    "    df['Date'] = pd.to_datetime(df['Timestamp']).dt.date\n",
    "    date_set = set(daily_data['Date'])\n",
    "    df['Closest Date'] = df['Date'].apply(lambda x: find_closest_date(x, date_set))\n",
    "    \n",
    "    merged_df = pd.merge(df, daily_data, left_on='Closest Date', right_on='Date', how='left')\n",
    "\n",
    "    # 플롯\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    ax1.plot(merged_df['Timestamp'], merged_df['Total Frequency'], color='tab:blue', label='Total Frequency')\n",
    "    ax1.set_xlabel('Timestamp')\n",
    "    ax1.set_ylabel('Total Frequency', color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    # 빈도의 gradient를 계산하여 그래프로 표시합니다.\n",
    "    gradient = np.gradient(merged_df['Total Frequency'])\n",
    "    ax1.plot(merged_df['Timestamp'], gradient, color='tab:orange', linestyle='--', label='Gradient')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.bar(merged_df['Timestamp'], merged_df['volume'], color='tab:red', alpha=0.1, label='Volume', width=3)\n",
    "    ax2.set_ylabel('Volume', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))  # 다른 축과 분리\n",
    "    ax3.plot(merged_df['Timestamp'], merged_df['open_avg'], color='tab:green', label='SPY Open Avg')\n",
    "    ax3.set_ylabel('SPY Open Avg', color='tab:green')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.title('Total Frequency, SPY Open Average, and Volume over Time')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 함수 호출\n",
    "plot_total_frequency_with_spy(topics_over_time, daily_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f1a2d-f0db-4009-b677-50e44f977e9e",
   "metadata": {},
   "source": [
    "### Gradient 끼리만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a2dbe-4a65-4fdd-bfde-5ca81f0a0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Timestamp를 datetime 형식으로 변환\n",
    "spy_data['Timestamp'] = pd.to_datetime(spy_data['timestamp'])\n",
    "\n",
    "# 날짜별로 그룹화하여 필요한 값 계산\n",
    "spy_data['Date'] = spy_data['Timestamp'].dt.date\n",
    "daily_data = spy_data.groupby('Date').agg({\n",
    "    'open': ['first', 'last'],\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'volume': 'sum'\n",
    "})\n",
    "\n",
    "# 컬럼명 정리\n",
    "daily_data.columns = ['open_first', 'open_last', 'high', 'low', 'volume']\n",
    "daily_data['open_avg'] = daily_data[['open_first', 'open_last']].mean(axis=1)\n",
    "daily_data.reset_index(inplace=True)\n",
    "\n",
    "# 날짜별 SPY 데이터가 없는 경우 가장 가까운 날짜의 데이터를 찾는 함수\n",
    "def find_closest_date(target_date, date_set):\n",
    "    while target_date not in date_set:\n",
    "        target_date -= timedelta(days=1)\n",
    "    return target_date\n",
    "\n",
    "# 토픽 빈도 데이터와 날짜별 open_avg 및 volume 데이터 병합 및 플롯\n",
    "def plot_gradients_with_spy(topics_over_time, daily_data, top_n_topics=20):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 토픽별 빈도를 저장할 딕셔너리 생성\n",
    "    topic_frequencies = {}\n",
    "\n",
    "    # 상위 n개의 토픽만을 사용하여 빈도를 더합니다.\n",
    "    for topic_id in range(1, top_n_topics + 1):\n",
    "        topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "        timestamps = topic_data['Timestamp']\n",
    "        frequencies = topic_data['Frequency']\n",
    "        \n",
    "        # 각 타임스탬프에서의 빈도를 더합니다.\n",
    "        for timestamp, frequency in zip(timestamps, frequencies):\n",
    "            if timestamp in topic_frequencies:\n",
    "                topic_frequencies[timestamp] += frequency\n",
    "            else:\n",
    "                topic_frequencies[timestamp] = frequency\n",
    "\n",
    "    # 딕셔너리를 데이터프레임으로 변환하여 그래프를 그립니다.\n",
    "    df = pd.DataFrame(list(topic_frequencies.items()), columns=['Timestamp', 'Total Frequency'])\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    \n",
    "    # 날짜별로 정리된 SPY 데이터와 병합\n",
    "    df['Date'] = pd.to_datetime(df['Timestamp']).dt.date\n",
    "    date_set = set(daily_data['Date'])\n",
    "    df['Closest Date'] = df['Date'].apply(lambda x: find_closest_date(x, date_set))\n",
    "    \n",
    "    merged_df = pd.merge(df, daily_data, left_on='Closest Date', right_on='Date', how='left')\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # open_avg_gradient를 계산하여 그래프로 표시합니다.\n",
    "    open_avg_gradient = np.zeros(len(merged_df))\n",
    "    for i in range(len(merged_df)):\n",
    "        start_idx = max(0, i - 4)\n",
    "        end_idx = min(len(merged_df), i + 5)\n",
    "        available_data_count = end_idx - start_idx\n",
    "        if available_data_count < 9:\n",
    "            scaling_factor = 9 / available_data_count\n",
    "            open_avg_gradient[i] = scaling_factor * np.abs(merged_df['open_avg'][end_idx - 1] - merged_df['open_avg'][start_idx])\n",
    "        else:\n",
    "            open_avg_gradient[i] = np.abs(merged_df['open_avg'][end_idx - 1] - merged_df['open_avg'][start_idx])\n",
    "\n",
    "    ax1.plot(merged_df['Timestamp'], open_avg_gradient - np.mean(open_avg_gradient), color='tab:purple', linestyle='--', label='Open Avg Gradient')\n",
    "    ax1.set_xlabel('Timestamp')\n",
    "    ax1.set_ylabel('Open Avg Gradient', color='tab:purple')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:purple')\n",
    "\n",
    "    # Frequency gradient를 계산하여 그래프로 표시합니다.\n",
    "    gradient = np.gradient(merged_df['Total Frequency'])\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(merged_df['Timestamp'], gradient - np.mean(gradient), color='tab:orange', linestyle='--', label='Frequency Gradient')\n",
    "    ax2.set_ylabel('Frequency Gradient', color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Volume gradient를 계산하여 그래프로 표시합니다.\n",
    "    volume_gradient = np.gradient(merged_df['volume'])\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 120))\n",
    "    ax3.plot(merged_df['Timestamp'], volume_gradient - np.mean(volume_gradient), color='tab:cyan', linestyle='--', label='Volume Gradient')\n",
    "    ax3.set_ylabel('Volume Gradient', color='tab:cyan')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:cyan')\n",
    "\n",
    "    # y축의 0 위치를 동일하게 조정합니다.\n",
    "    ax1.set_ylim(-max(abs(open_avg_gradient)), max(abs(open_avg_gradient)))\n",
    "    ax2.set_ylim(-max(abs(gradient)), max(abs(gradient)))\n",
    "    ax3.set_ylim(-max(abs(volume_gradient)), max(abs(volume_gradient)))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.title('Open Avg Gradient, Frequency Gradient, and Volume Gradient over Time')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 함수 호출\n",
    "plot_gradients_with_spy(topics_over_time, daily_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2fe87-58c7-4aae-a350-79c4b894a248",
   "metadata": {},
   "source": [
    "### Gradient(open avg로 모든 날의 gradient를 생략없이 합산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffbb55-f8e3-4436-a4da-cf98c3883a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Timestamp를 datetime 형식으로 변환\n",
    "spy_data['Timestamp'] = pd.to_datetime(spy_data['timestamp'])\n",
    "\n",
    "# 날짜별로 그룹화하여 필요한 값 계산\n",
    "spy_data['Date'] = spy_data['Timestamp'].dt.date\n",
    "daily_data = spy_data.groupby('Date').agg({\n",
    "    'open': ['first', 'last'],\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'volume': 'sum'\n",
    "})\n",
    "\n",
    "# 컬럼명 정리\n",
    "daily_data.columns = ['open_first', 'open_last', 'high', 'low', 'volume']\n",
    "daily_data['open_avg'] = daily_data[['open_first', 'open_last']].mean(axis=1)\n",
    "\n",
    "# 'difference' 컬럼 추가: 가장 이른 시간 open과 가장 늦은 시간 close의 차이(절댓값)\n",
    "daily_data['difference'] = abs(daily_data['open_first'] - daily_data['open_last'])\n",
    "\n",
    "# 날짜별 SPY 데이터가 없는 경우 가장 가까운 날짜의 데이터를 찾는 함수\n",
    "def find_closest_date(target_date, date_set):\n",
    "    while target_date not in date_set:\n",
    "        target_date -= timedelta(days=1)\n",
    "    return target_date\n",
    "\n",
    "# 토픽 빈도 데이터와 날짜별 difference 데이터 병합 및 플롯\n",
    "def plot_gradients_with_spy(topics_over_time, daily_data, top_n_topics=20):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 토픽별 빈도를 저장할 딕셔너리 생성\n",
    "    topic_frequencies = {}\n",
    "\n",
    "    # 상위 n개의 토픽만을 사용하여 빈도를 더합니다.\n",
    "    for topic_id in range(1, top_n_topics + 1):\n",
    "        topic_data = topics_over_time[topics_over_time['Topic'] == topic_id]\n",
    "        timestamps = topic_data['Timestamp']\n",
    "        frequencies = topic_data['Frequency']\n",
    "        \n",
    "        # 각 타임스탬프에서의 빈도를 더합니다.\n",
    "        for timestamp, frequency in zip(timestamps, frequencies):\n",
    "            if timestamp in topic_frequencies:\n",
    "                topic_frequencies[timestamp] += frequency\n",
    "            else:\n",
    "                topic_frequencies[timestamp] = frequency\n",
    "\n",
    "    # 딕셔너리를 데이터프레임으로 변환하여 그래프를 그립니다.\n",
    "    df = pd.DataFrame(list(topic_frequencies.items()), columns=['Timestamp', 'Total Frequency'])\n",
    "    df = df.sort_values(by='Timestamp')\n",
    "    \n",
    "    # 날짜별로 정리된 SPY 데이터와 병합\n",
    "    df['Date'] = pd.to_datetime(df['Timestamp']).dt.date\n",
    "    date_set = set(daily_data.index)\n",
    "    df['Closest Date'] = df['Date'].apply(lambda x: find_closest_date(x, date_set))\n",
    "    \n",
    "    merged_df = pd.merge(df, daily_data, left_on='Closest Date', right_index=True, how='left')\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # open_avg_gradient를 계산하여 그래프로 표시합니다.\n",
    "    open_avg_gradient = np.zeros(len(merged_df))\n",
    "    for i in range(len(merged_df)):\n",
    "        start_idx = max(0, i - 4)\n",
    "        end_idx = min(len(merged_df), i + 5)\n",
    "        available_data_count = end_idx - start_idx\n",
    "        if available_data_count < 9:\n",
    "            scaling_factor = 9 / available_data_count\n",
    "            open_avg_gradient[i] = scaling_factor * np.sum(merged_df['difference'][start_idx:end_idx])\n",
    "        else:\n",
    "            open_avg_gradient[i] = np.sum(merged_df['difference'][start_idx:end_idx])\n",
    "\n",
    "    ax1.plot(merged_df['Timestamp'], open_avg_gradient, color='tab:purple', linestyle='--', label='Open Avg Gradient')\n",
    "    ax1.set_xlabel('Timestamp')\n",
    "    ax1.set_ylabel('Open Avg Gradient', color='tab:purple')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:purple')\n",
    "\n",
    "    # Frequency gradient를 계산하여 그래프로 표시합니다.\n",
    "    gradient = np.gradient(merged_df['Total Frequency'])\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(merged_df['Timestamp'], gradient, color='tab:orange', linestyle='--', label='Frequency Gradient')\n",
    "    ax2.set_ylabel('Frequency Gradient', color='tab:orange')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Volume gradient를 계산하여 그래프로 표시합니다.\n",
    "    volume_gradient = np.gradient(merged_df['volume'])\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 120))\n",
    "    ax3.plot(merged_df['Timestamp'], volume_gradient, color='tab:cyan', linestyle='--', label='Volume Gradient')\n",
    "    ax3.set_ylabel('Volume Gradient', color='tab:cyan')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:cyan')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.legend(loc='upper left')\n",
    "    plt.title('Open Avg Gradient, Frequency Gradient, and Volume Gradient over Time')\n",
    "    plt.show()\n",
    "\n",
    "# 함수 호출\n",
    "plot_gradients_with_spy(topics_over_time, daily_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
